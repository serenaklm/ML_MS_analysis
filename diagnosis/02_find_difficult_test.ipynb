{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pprint import pprint\n",
    "from utils import load_pickle, load_json, pickle_data\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_folder = \"./cache\"\n",
    "if not os.path.exists(cache_folder): os.makedirs(cache_folder)\n",
    "\n",
    "THRESHOLD = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(results, method1, method2):\n",
    "\n",
    "    loss_method1, loss_method2 = [],[]\n",
    "    rank_method1, rank_method2 = [],[]\n",
    "\n",
    "    for _, v in results.items(): \n",
    "\n",
    "        if method1 in v and method2 in v:\n",
    "            loss_method1.append(v[method1][0])\n",
    "            loss_method2.append(v[method2][0])\n",
    "            rank_method1.append(v[method1][1])\n",
    "            rank_method2.append(v[method2][1])\n",
    "\n",
    "    rank_correlation = round(float(stats.spearmanr(rank_method1, rank_method2).statistic), 3)\n",
    "    linear_correlation = round(float(stats.pearsonr(loss_method1, loss_method2).statistic), 3)\n",
    "\n",
    "    return rank_correlation, linear_correlation, len(loss_method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get samples that appear across all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nist2023 2926\n",
      "nist2020 3052\n",
      "massspecgym 387\n",
      "canopus 65\n"
     ]
    }
   ],
   "source": [
    "repeated_test_ids_path = os.path.join(cache_folder, \"repeated_test_ids.pkl\")\n",
    "\n",
    "if not os.path.exists(repeated_test_ids_path):\n",
    "\n",
    "    splits_folder = \"/data/rbg/users/klingmin/projects/MS_processing/data_splits\"\n",
    "    repeated_test_ids = {}\n",
    "\n",
    "    for dataset in os.listdir(splits_folder):\n",
    "        \n",
    "        repeated_test_ids[dataset] = {}\n",
    "        all_ids = None\n",
    "\n",
    "        for splits in os.listdir(os.path.join(splits_folder, dataset, \"splits\")):\n",
    "            \n",
    "            if \"CF\" in splits: continue\n",
    "\n",
    "            current_ids = load_json(os.path.join(splits_folder, dataset, \"splits\", splits))[\"test\"]\n",
    "\n",
    "            if all_ids == None: all_ids = set(current_ids)\n",
    "            all_ids = all_ids.intersection(current_ids)\n",
    "\n",
    "        repeated_test_ids[dataset] = all_ids\n",
    "    \n",
    "    pickle_data(repeated_test_ids, repeated_test_ids_path)\n",
    "\n",
    "repeated_test_ids = load_pickle(repeated_test_ids_path)\n",
    "\n",
    "for dataset, rec in repeated_test_ids.items():\n",
    "    print(dataset, len(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"/data/rbg/users/klingmin/projects/ML_MS_analysis/FP_prediction/baseline_models/models_cached/morgan4_4096\"\n",
    "mist_results_folder = \"/data/rbg/users/klingmin/projects/ML_MS_analysis/FP_prediction/mist/models_cached/morgan4_4096\"\n",
    "merged_results_path = os.path.join(cache_folder, \"merged_test_results.pkl\")\n",
    "\n",
    "if not os.path.exists(merged_results_path):\n",
    "\n",
    "    merged_results = {} \n",
    "\n",
    "    for folder in [results_folder, mist_results_folder]:\n",
    "\n",
    "        for dataset in os.listdir(folder):\n",
    "\n",
    "            dataset_folder = os.path.join(folder, dataset)\n",
    "\n",
    "            if dataset not in merged_results: merged_results[dataset] = {} \n",
    "\n",
    "            for f in os.listdir(dataset_folder):\n",
    "\n",
    "                test_filepath = os.path.join(dataset_folder, f, \"test_results.pkl\")\n",
    "                test_results = load_pickle(test_filepath)\n",
    "                test_scores = [v[\"loss\"] for _, v in test_results.items()]\n",
    "                test_scores = sorted(test_scores, reverse = False) # Ascending order\n",
    "\n",
    "                # Add to all results\n",
    "                for k, v in test_results.items():\n",
    "                    if torch.is_tensor(k): k = k.item()\n",
    "\n",
    "                    if k not in merged_results[dataset]: merged_results[dataset][k] = {}\n",
    "                    merged_results[dataset][k][f] = (v[\"loss\"], test_scores.index(v[\"loss\"]) / len(test_scores))\n",
    "    \n",
    "    pickle_data(merged_results, merged_results_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    merged_results = load_pickle(merged_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test sets that are consistently bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_sieved = {}\n",
    "\n",
    "for dataset, results in merged_results.items():\n",
    "\n",
    "    merged_results_sieved[dataset] = {}\n",
    "    selected_ids = repeated_test_ids[dataset]\n",
    "\n",
    "    for id_, scores in results.items():\n",
    "\n",
    "        if f\"{id_}.pkl\" in selected_ids: \n",
    "            merged_results_sieved[dataset][id_] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because sorted = False ==> ascending order ==> bigger the rank, more difficult the example\n",
    "datasets = [\"massspecgym\", \"canopus\", \"nist2023\"]\n",
    "models = [\"binned_4096\", \"mist_4096\", \"MS_4096\", \"mist\"]\n",
    "kept_set = {d : {} for d in datasets} \n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        id_list = [] \n",
    "\n",
    "        for k, s_pair in merged_results_sieved[dataset].items():\n",
    "            \n",
    "            sieved_s_pair = [p[1] > THRESHOLD for _, p in s_pair.items() if model in _]\n",
    "            if True not in sieved_s_pair: continue \n",
    "            id_list.append(str(k))\n",
    "        \n",
    "        kept_set[dataset][model] = list(set(id_list))\n",
    "    \n",
    "    # Get difficult test samples across all models \n",
    "    current_id_list = list(set(id_list)) \n",
    "    for model in models: \n",
    "        current_id_list = [i for i in current_id_list if i in kept_set[dataset][model]]\n",
    "    \n",
    "    kept_set[dataset][\"all\"] = current_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "massspecgym binned_4096 25.1 387\n",
      "massspecgym mist_4096 17.3 387\n",
      "massspecgym MS_4096 26.1 387\n",
      "massspecgym mist 17.3 387\n",
      "massspecgym all 9.0 387\n",
      "\n",
      "canopus binned_4096 10.8 65\n",
      "canopus mist_4096 7.7 65\n",
      "canopus MS_4096 21.5 65\n",
      "canopus mist 7.7 65\n",
      "canopus all 0.0 65\n",
      "\n",
      "nist2023 binned_4096 0.0 2926\n",
      "nist2023 mist_4096 5.1 2926\n",
      "nist2023 MS_4096 0.0 2926\n",
      "nist2023 mist 5.1 2926\n",
      "nist2023 all 0.0 2926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset, rec in kept_set.items():\n",
    "\n",
    "    for model, id_list in rec.items():\n",
    "        \n",
    "        print(dataset, model, round(len(id_list) / len(repeated_test_ids[dataset]) * 100, 1),  len(repeated_test_ids[dataset]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
