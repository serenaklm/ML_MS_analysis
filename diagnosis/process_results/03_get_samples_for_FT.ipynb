{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1449bb",
   "metadata": {},
   "source": [
    "`` This script designs various strategies to sample training data using the influence function scores ``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5813904",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013d5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "from utils import load_pickle, pickle_data, load_json, write_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dda42a",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87953f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage of samples that we will be using for training\n",
    "ratio_list = [0.05, 0.10, 0.20, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "\n",
    "# Get all the required folders\n",
    "splits_folder = \"/data/rbg/users/klingmin/projects/MS_processing/data_splits/\"\n",
    "frags_folder = \"/data/rbg/users/klingmin/projects/MS_processing/data/\"\n",
    "CF_folder = \"/data/rbg/users/klingmin/projects/MS_processing/CFs\"\n",
    "baseline_folder = \"/data/rbg/users/klingmin/projects/ML_MS_analysis/FP_prediction/baseline_models/best_models\"\n",
    "mist_folder = \"/data/rbg/users/klingmin/projects/ML_MS_analysis/FP_prediction/mist/best_models\"\n",
    "results_folders = [baseline_folder, mist_folder]\n",
    "\n",
    "# Get the considered datasets \n",
    "datasets = [\"massspecgym\", \"nist2023\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b086166",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f303128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_train_CF_oov(OOV_list, train_ids, frags_folder):\n",
    "\n",
    "    scores = {} \n",
    "    for id_ in tqdm(train_ids):\n",
    "\n",
    "        frags = [f[\"comment\"][\"f_pred\"] for f in load_pickle(os.path.join(frags_folder, id_))[\"peaks\"]]\n",
    "        frags = list(set([f for f in frags if f != \"\"]))\n",
    "\n",
    "        score = sum([f in OOV_list for f in frags])\n",
    "        scores[id_] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e431e7",
   "metadata": {},
   "source": [
    "`` Get upper bound performance ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# UB 1: FT on all available FT data \n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    split_file = load_json(os.path.join(splits_folder, dataset, \"splits\", \"sampling_split.json\"))\n",
    "    output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"random\", f\"sampled_random_100.json\")\n",
    "\n",
    "    new_split = {\"train\": split_file[\"train\"], \n",
    "                 \"val\": split_file[\"val\"],\n",
    "                 \"test\": split_file[\"test\"]}\n",
    "    \n",
    "    write_json(new_split, output_path)\n",
    "        \n",
    "    # Add full set \n",
    "    original_train = load_json(os.path.join(splits_folder, dataset, \"splits\", \"scaffold_vanilla_sieved.json\"))[\"train\"]\n",
    "    output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"random\", f\"sampled_random_100_combined.json\")\n",
    "    full_set = {\"train\": original_train + split_file[\"train\"],\n",
    "                \"val\": split_file[\"val\"], \n",
    "                \"test\": split_file[\"test\"]}\n",
    "\n",
    "    write_json(full_set, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181089f",
   "metadata": {},
   "source": [
    "`` Sampling stategy 1: sample randomly ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84c7ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 19418.07it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 16029.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.92it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.25it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  7.25it/s]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# V1: sample randomly \n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    output_split_folder = os.path.join(splits_folder, dataset, \"splits_sampling\", \"random\")\n",
    "    split_file = load_json(os.path.join(splits_folder, dataset, \"splits\", \"sampling_split.json\"))\n",
    "\n",
    "    train_ids, val_ids, test_ids = split_file[\"train\"], split_file[\"val\"], split_file[\"test\"]\n",
    "\n",
    "    if not os.path.exists(output_split_folder): os.makedirs(output_split_folder)\n",
    "\n",
    "    for ratio in tqdm(ratio_list):\n",
    "\n",
    "        output_path = os.path.join(output_split_folder, f\"sampled_random_{int(ratio*100)}.json\")\n",
    "        if os.path.exists(output_path): continue \n",
    "\n",
    "        train_ids_sampled = random.sample(train_ids, int(len(train_ids) * ratio))\n",
    "\n",
    "        new_split = {\"train\": train_ids_sampled, \n",
    "                     \"val\": val_ids,\n",
    "                     \"test\": test_ids}\n",
    "\n",
    "        write_json(new_split, output_path)\n",
    "    \n",
    "# V2: sample randomly + add original data \n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    original_train = load_json(os.path.join(splits_folder, dataset, \"splits\", \"scaffold_vanilla_sieved.json\"))[\"train\"]\n",
    "\n",
    "    for ratio in tqdm(ratio_list):\n",
    "\n",
    "        sampled_split = load_json(os.path.join(splits_folder, dataset, \"splits_sampling\", \"random\", f\"sampled_random_{int(ratio*100)}.json\"))\n",
    "        output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"random\", f\"sampled_random_{int(ratio*100)}_combined.json\")\n",
    "\n",
    "        if os.path.exists(output_path): continue\n",
    "        new_split = {\"train\": sampled_split[\"train\"] + original_train, \n",
    "                     \"val\": sampled_split[\"val\"],\n",
    "                     \"test\": sampled_split[\"test\"]}\n",
    "        \n",
    "        write_json(new_split, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b1d90",
   "metadata": {},
   "source": [
    "`` Sampling strategy 2: sample based on CF ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c969f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 10.12it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  7.93it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  8.10it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  8.72it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# # V1: sample based on CF\n",
    "# for dataset in tqdm(datasets):\n",
    "\n",
    "#     output_split_folder = os.path.join(splits_folder, dataset, \"splits_sampling\", \"CF\")\n",
    "#     split_file = load_json(os.path.join(splits_folder, dataset, \"splits\", \"sampling_split.json\"))\n",
    "    \n",
    "#     current_CF_folder = os.path.join(CF_folder, dataset, \"scaffold_vanilla_sieved_split\")\n",
    "#     current_frags_folder = os.path.join(frags_folder, dataset, \"frags_preds\")\n",
    "#     train_CFs, test_CFs = load_pickle(os.path.join(current_CF_folder, \"train_CFs.pkl\")), load_pickle(os.path.join(current_CF_folder, \"test_CFs.pkl\"))\n",
    "#     train_mist_CFs, test_mist_CFs = load_pickle(os.path.join(current_CF_folder, \"train_MIST_CFs.pkl\")), load_pickle(os.path.join(current_CF_folder, \"test_MIST_CFs.pkl\"))\n",
    "\n",
    "#     # Get the OOVs for this set \n",
    "#     OOV = test_CFs - train_CFs\n",
    "#     OOV_mist = test_mist_CFs - train_mist_CFs\n",
    "\n",
    "#     train_ids, val_ids, test_ids = split_file[\"train\"], split_file[\"val\"], split_file[\"test\"]\n",
    "#     if not os.path.exists(output_split_folder): os.makedirs(output_split_folder)\n",
    "\n",
    "#     # Get the percent of peaks with CFs in the OOV list for train and sample based on that\n",
    "#     train_scores = score_train_CF_oov(OOV, train_ids, current_frags_folder)\n",
    "#     train_scores_mist = score_train_CF_oov(OOV_mist, train_ids, current_frags_folder)\n",
    "\n",
    "#     train_scores = sorted(train_scores.items(), key = lambda x: x[1], reverse = True)\n",
    "#     train_scores_mist = sorted(train_scores_mist.items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "#     for ratio in tqdm(ratio_list):\n",
    "\n",
    "#         output_path = os.path.join(output_split_folder, f\"sampled_CF_{int(ratio*100)}.json\")\n",
    "#         output_MIST_path = os.path.join(output_split_folder, f\"sampled_CF_MIST_{int(ratio*100)}.json\")\n",
    "\n",
    "#         if os.path.exists(output_path): \n",
    "#             continue \n",
    "#         else:\n",
    "#             train_ids_sampled = [t[0] for t in train_scores[:int(len(train_ids) * ratio)]]\n",
    "#             new_split = {\"train\": train_ids_sampled, \n",
    "#                          \"val\": val_ids,\n",
    "#                          \"test\": test_ids}\n",
    "\n",
    "#             write_json(new_split, output_path)\n",
    "\n",
    "#         if os.path.exists(output_MIST_path): \n",
    "#             continue \n",
    "#         else:\n",
    "#             train_ids_sampled = [t[0] for t in train_scores_mist[:int(len(train_ids) * ratio)]]\n",
    "#             new_split = {\"train\": train_ids_sampled, \n",
    "#                          \"val\": val_ids,\n",
    "#                          \"test\": test_ids}\n",
    "\n",
    "#             write_json(new_split, output_MIST_path)\n",
    "\n",
    "# V2: sample based on CF + add original data \n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    original_train = load_json(os.path.join(splits_folder, dataset, \"splits\", \"scaffold_vanilla_sieved.json\"))[\"train\"]\n",
    "\n",
    "    for ratio in tqdm(ratio_list):\n",
    "\n",
    "        sampled_split = load_json(os.path.join(splits_folder, dataset, \"splits_sampling\", \"CF\", f\"sampled_CF_{int(ratio*100)}.json\"))\n",
    "        output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"CF\", f\"sampled_CF_{int(ratio*100)}_combined.json\")\n",
    "        if os.path.exists(output_path): continue \n",
    "\n",
    "        new_split = {\"train\": sampled_split[\"train\"] + original_train, \n",
    "                     \"val\": sampled_split[\"val\"],\n",
    "                     \"test\": sampled_split[\"test\"]}\n",
    "        \n",
    "        write_json(new_split, output_path)\n",
    "\n",
    "    # MIST version \n",
    "    for ratio in tqdm(ratio_list):\n",
    "\n",
    "        sampled_split = load_json(os.path.join(splits_folder, dataset, \"splits_sampling\", \"CF\", f\"sampled_CF_MIST_{int(ratio*100)}.json\"))\n",
    "        mist_output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"CF\", f\"sampled_CF_MIST_{int(ratio*100)}_combined.json\")\n",
    "        if os.path.exists(mist_output_path): continue \n",
    "\n",
    "        new_split = {\"train\": sampled_split[\"train\"] + original_train, \n",
    "                     \"val\": sampled_split[\"val\"],\n",
    "                     \"test\": sampled_split[\"test\"]}\n",
    "        \n",
    "        write_json(new_split, mist_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d02d1",
   "metadata": {},
   "source": [
    "`` Sampling strategy 3: sample based on influence score ``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc493942",
   "metadata": {},
   "source": [
    "`` 3a: Using a separate validation set to select training data ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9a1b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 21.96it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 26.51it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 23.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.39it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 15401.36it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 41971.02it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 42027.09it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.18it/s]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# # V1: sample based on IF (val) \n",
    "# for dataset in tqdm(datasets):\n",
    "    \n",
    "#     if dataset == \"massspecgym\": continue\n",
    "    \n",
    "#     output_split_folder = os.path.join(splits_folder, dataset, \"splits_sampling\", \"IF_val\")\n",
    "#     split_file = load_json(os.path.join(splits_folder, dataset, \"splits\", \"sampling_split.json\"))\n",
    "#     val_ids, test_ids = split_file[\"val\"], split_file[\"test\"]\n",
    "\n",
    "#     if not os.path.exists(output_split_folder): os.makedirs(output_split_folder)\n",
    "\n",
    "#     for folder in results_folders:\n",
    "\n",
    "#         current_result_folder = os.path.join(folder, f\"{dataset}_sieved\")\n",
    "#         all_checkpoints = [os.path.join(current_result_folder, f) for f in os.listdir(current_result_folder) if \"scaffold\" in f]\n",
    "\n",
    "#         for checkpoint in all_checkpoints:\n",
    "\n",
    "#             if \"EK-FAC_scores_for_sampling_val.pkl\" not in os.listdir(checkpoint): continue \n",
    "\n",
    "#             model_name = checkpoint.split(\"/\")[-1].split(\"_\")[1]\n",
    "#             influence_scores = load_pickle(os.path.join(checkpoint, \"EK-FAC_scores_for_sampling_val.pkl\"))[\"all_modules\"]\n",
    "#             IF_scores_aggregated = torch.sum(influence_scores, dim = 0)\n",
    "#             train_ids_FT = load_pickle(os.path.join(checkpoint, \"train_ids_FT.pkl\"))\n",
    "            \n",
    "#             for ratio in tqdm(ratio_list):\n",
    "\n",
    "#                 output_path = os.path.join(output_split_folder, f\"sampled_IF_val_{model_name}_{int(ratio*100)}.json\")\n",
    "#                 if os.path.exists(output_path): continue \n",
    "#                 print(f\"Processing {output_path} now\")\n",
    "#                 train_idx_sampled = IF_scores_aggregated.topk(k = int(ratio * IF_scores_aggregated.shape[0])).indices\n",
    "#                 train_ids_sampled = [train_ids_FT[i] for i in train_idx_sampled]\n",
    "\n",
    "#                 new_split = {\"train\": train_ids_sampled, \n",
    "#                             \"val\": val_ids,\n",
    "#                             \"test\": test_ids}\n",
    "\n",
    "#                 write_json(new_split, output_path)\n",
    "\n",
    "# V2: sample based on IF (val) + add original data \n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    original_train = load_json(os.path.join(splits_folder, dataset, \"splits\", \"scaffold_vanilla_sieved.json\"))[\"train\"]\n",
    "\n",
    "    for model_name in [\"binned\", \"formula\", \"MS\", \"MIST\"]:\n",
    "\n",
    "        for ratio in tqdm(ratio_list):\n",
    "\n",
    "            sampled_split = os.path.join(splits_folder, dataset, \"splits_sampling\", \"IF_val\", f\"sampled_IF_val_{model_name}_{int(ratio*100)}.json\")\n",
    "            if not os.path.exists(sampled_split): continue\n",
    "            sampled_split = load_json(sampled_split)\n",
    "            output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"IF_val\",  f\"sampled_IF_val_{model_name}_{int(ratio*100)}_combined.json\")\n",
    "            # if os.path.exists(output_path): continue \n",
    "\n",
    "            new_split = {\"train\": sampled_split[\"train\"] + original_train, \n",
    "                        \"val\": sampled_split[\"val\"],\n",
    "                        \"test\": sampled_split[\"test\"]}\n",
    "            \n",
    "            write_json(new_split, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb171c",
   "metadata": {},
   "source": [
    "`` 3b: Using the test set to select training data directly ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d084d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.55it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 17.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 15.27it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  9.02it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2757.48it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3278.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3504.21it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  7.19it/s]\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# for dataset in tqdm(datasets):\n",
    "\n",
    "#     if dataset == \"massspecgym\": continue\n",
    "    \n",
    "#     output_split_folder = os.path.join(splits_folder, dataset, \"splits_sampling\", \"IF_test\")\n",
    "#     split_file = load_json(os.path.join(splits_folder, dataset, \"splits\", \"sampling_split.json\"))\n",
    "#     val_ids, test_ids = split_file[\"val\"], split_file[\"test\"]\n",
    "\n",
    "#     if not os.path.exists(output_split_folder): os.makedirs(output_split_folder)\n",
    "\n",
    "#     for folder in results_folders:\n",
    "\n",
    "#         current_result_folder = os.path.join(folder, f\"{dataset}_sieved\")\n",
    "#         all_checkpoints = [os.path.join(current_result_folder, f) for f in os.listdir(current_result_folder) if \"scaffold\" in f]\n",
    "\n",
    "#         for checkpoint in all_checkpoints:\n",
    "\n",
    "#             if \"EK-FAC_scores_for_sampling_test.pkl\" not in os.listdir(checkpoint): continue \n",
    "\n",
    "#             model_name = checkpoint.split(\"/\")[-1].split(\"_\")[1]\n",
    "#             influence_scores = load_pickle(os.path.join(checkpoint, \"EK-FAC_scores_for_sampling_test.pkl\"))[\"all_modules\"]\n",
    "#             IF_scores_aggregated = torch.sum(influence_scores, dim = 0)\n",
    "#             train_ids_FT = load_pickle(os.path.join(checkpoint, \"train_ids_FT.pkl\"))\n",
    "                        \n",
    "#             for ratio in tqdm(ratio_list):\n",
    "\n",
    "#                 output_path = os.path.join(output_split_folder, f\"sampled_IF_test_{model_name}_{int(ratio*100)}.json\")\n",
    "#                 if os.path.exists(output_path): continue \n",
    "#                 print(f\"Processing {output_path} now\")\n",
    "                \n",
    "#                 train_idx_sampled = IF_scores_aggregated.topk(k = int(ratio * IF_scores_aggregated.shape[0])).indices\n",
    "#                 train_ids_sampled = [train_ids_FT[i] for i in train_idx_sampled]\n",
    "\n",
    "#                 new_split = {\"train\": train_ids_sampled, \n",
    "#                             \"val\": val_ids,\n",
    "#                             \"test\": test_ids}\n",
    "\n",
    "#                 write_json(new_split, output_path)\n",
    "\n",
    "# V2: sample based on IF (test) + add original data \n",
    "for dataset in tqdm(datasets):\n",
    "\n",
    "    original_train = load_json(os.path.join(splits_folder, dataset, \"splits\", \"scaffold_vanilla_sieved.json\"))[\"train\"]\n",
    "\n",
    "    for model_name in [\"binned\", \"formula\", \"MS\", \"MIST\"]:\n",
    "\n",
    "        for ratio in tqdm(ratio_list):\n",
    "\n",
    "            sampled_split = os.path.join(splits_folder, dataset, \"splits_sampling\", \"IF_test\", f\"sampled_IF_test_{model_name}_{int(ratio*100)}.json\")\n",
    "            if not os.path.exists(sampled_split): continue\n",
    "            sampled_split = load_json(sampled_split)\n",
    "            output_path = os.path.join(splits_folder, dataset, \"splits_sampling\", \"IF_test\", f\"sampled_IF_test_{model_name}_{int(ratio*100)}_combined.json\")\n",
    "            # if os.path.exists(output_path): continue \n",
    "\n",
    "            new_split = {\"train\": sampled_split[\"train\"] + original_train, \n",
    "                        \"val\": sampled_split[\"val\"],\n",
    "                        \"test\": sampled_split[\"test\"]}\n",
    "            \n",
    "            write_json(new_split, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff354232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
